{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 257,
      "id": "9d2555a5-bbe5-4d2f-9a25-1c6cc6f255d1",
      "metadata": {
        "id": "9d2555a5-bbe5-4d2f-9a25-1c6cc6f255d1"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "import nltk\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "import pprint\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import time\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import dotenv\n",
        "import json\n",
        "import torch\n",
        "import tqdm\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dotenv.load_dotenv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohJsxg0yQbkG",
        "outputId": "f094b8b5-a5c8-4871-8190-ac33bf4a6b9e"
      },
      "id": "ohJsxg0yQbkG",
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = os.getenv(\"API_KEY\")\n",
        "client = OpenAI(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "orHq9AMGuia9"
      },
      "id": "orHq9AMGuia9",
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Baixando o modelo 'punkt' do NLTK...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahys9FznMU43",
        "outputId": "d9ed7ccd-0f73-4997-f843-0dfd29cc4749"
      },
      "id": "Ahys9FznMU43",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "86f4c70d-624c-4d75-aafa-21f4646a3d5c",
      "metadata": {
        "id": "86f4c70d-624c-4d75-aafa-21f4646a3d5c"
      },
      "outputs": [],
      "source": [
        "model_name = 'juridics/bertimbau-base-portuguese-sts-scale'\n",
        "tokenizer_sentencas = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
        "\n",
        "model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processar_strings_longas(array_de_entrada, limite_de_tamanho=10):\n",
        "\n",
        "    array_de_saida = []\n",
        "\n",
        "    for texto in array_de_entrada:\n",
        "        if len(texto) > limite_de_tamanho:\n",
        "            novas_sentencas = tokenizer_sentencas.tokenize(texto)\n",
        "            for sentenca in novas_sentencas:\n",
        "              try:\n",
        "                saida = [palavra.strip() for palavra in sentenca.split(\":\") if palavra]\n",
        "                array_de_saida.extend(saida)\n",
        "              except:\n",
        "                array_de_saida.extend(novas_sentencas)\n",
        "\n",
        "        else:\n",
        "            array_de_saida.append(texto)\n",
        "\n",
        "    return array_de_saida"
      ],
      "metadata": {
        "id": "1o05rSWeK-lP"
      },
      "id": "1o05rSWeK-lP",
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centroid(bbox):\n",
        "    x0, y0, x1, y1 = bbox\n",
        "    cx = (x0 + x1) / 2\n",
        "    cy = (y0 + y1) / 2\n",
        "    return (round(cx), round(cy))"
      ],
      "metadata": {
        "id": "cheIbZavUp2C"
      },
      "id": "cheIbZavUp2C",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_closest_positions(array, positions_dict, k=5, tried=None):\n",
        "    if tried is None:\n",
        "        tried = set()\n",
        "\n",
        "    clean_positions = {str(key): (float(val[0]), float(val[1]))\n",
        "                       for key, val in positions_dict.items()}\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    keys = np.array(list(clean_positions.keys()))\n",
        "    coords = np.array(list(clean_positions.values()), dtype=float)\n",
        "\n",
        "    for name in array:\n",
        "\n",
        "        if name in tried:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            target_key = name\n",
        "            target = np.array(clean_positions[name], dtype=float)\n",
        "\n",
        "        except KeyError:\n",
        "            match = next((key for key in clean_positions if name in key), None)\n",
        "\n",
        "            if match:\n",
        "                tried.add(name)\n",
        "\n",
        "                target_key = match\n",
        "                target = np.array(clean_positions[match], dtype=float)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "        deltas = coords - target\n",
        "        distances = np.sqrt(np.sum(deltas * deltas, axis=1))\n",
        "\n",
        "        idx = np.argpartition(distances, k)[:k]\n",
        "\n",
        "        results[target_key] = [\n",
        "            (str(keys[i]), (float(coords[i][0]), float(coords[i][1])))\n",
        "            for i in idx\n",
        "        ]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Pcll6ADUhqdo"
      },
      "id": "Pcll6ADUhqdo",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f08b128d-2263-43cd-a0a1-73b7dd6d23d3",
      "metadata": {
        "id": "f08b128d-2263-43cd-a0a1-73b7dd6d23d3"
      },
      "source": [
        "### Extraindo todas as frases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "f9000078-8bae-4f3a-8103-aab4c2a164da",
      "metadata": {
        "id": "f9000078-8bae-4f3a-8103-aab4c2a164da"
      },
      "outputs": [],
      "source": [
        "def extrair_texto_arquivo(caminho_do_arquivo):\n",
        "  #Extraindo texto do arquivo\n",
        "  doc = fitz.open(caminho_do_arquivo)\n",
        "  page = doc.load_page(0)\n",
        "\n",
        "  text = page.get_text(\"text\")\n",
        "\n",
        "  texts = text.split(\"\\n\")\n",
        "\n",
        "  texts = [palavra.strip() for palavra in texts if palavra]\n",
        "  texts = [palavra.lower() for palavra in texts]\n",
        "  return texts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### String Cleaning"
      ],
      "metadata": {
        "id": "AFQqjrva-Ji7"
      },
      "id": "AFQqjrva-Ji7"
    },
    {
      "cell_type": "code",
      "source": [
        "def limpa_strings_simples(texts):\n",
        "  text_len = [len(text) for text in texts]\n",
        "  text_len = np.array(text_len)\n",
        "  avg_size = round(np.mean(text_len))\n",
        "  texts_clean = processar_strings_longas(texts, avg_size)\n",
        "  return texts_clean"
      ],
      "metadata": {
        "id": "NzElvs2J7pEc"
      },
      "id": "NzElvs2J7pEc",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_labels_words(labels_mapped_words, label, texts):\n",
        "  cache = labels_mapped_words.setdefault(label, set())\n",
        "\n",
        "  newly_added = []\n",
        "\n",
        "  for text in texts:\n",
        "      if text not in cache:\n",
        "          cache.add(text)\n",
        "          newly_added.append(text)\n",
        "\n",
        "  return newly_added"
      ],
      "metadata": {
        "id": "ge5XP8_MhcUA"
      },
      "id": "ge5XP8_MhcUA",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6NxS8dsH9pQ"
      },
      "id": "Q6NxS8dsH9pQ",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d2ac7871-dd7f-4e17-8c78-2a8eaab8d522",
      "metadata": {
        "id": "d2ac7871-dd7f-4e17-8c78-2a8eaab8d522"
      },
      "source": [
        "### Pegando as posições das frases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "rbfVuOcD_7TT",
      "metadata": {
        "id": "rbfVuOcD_7TT"
      },
      "outputs": [],
      "source": [
        "def get_words_pos(caminho_do_arquivo):\n",
        "  doc = fitz.open(caminho_do_arquivo)\n",
        "  page = doc.load_page(0)\n",
        "\n",
        "  data = page.get_text(\"dict\")\n",
        "\n",
        "  mapped_text_pos = {}\n",
        "\n",
        "  for  i,block in enumerate(data[\"blocks\"]):\n",
        "      if data[\"blocks\"][i][\"type\"] == 0:\n",
        "          for j, line in enumerate(data[\"blocks\"][i][\"lines\"]):\n",
        "              text = data[\"blocks\"][i][\"lines\"][j][\"spans\"][0][\"text\"]\n",
        "              bbox = data[\"blocks\"][i][\"lines\"][j][\"spans\"][0][\"bbox\"]\n",
        "              centroid = get_centroid(bbox)\n",
        "              text = text.strip()\n",
        "              text = text.lower()\n",
        "              mapped_text_pos[text] = centroid\n",
        "\n",
        "  return mapped_text_pos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_embeddings(words: list[str]):\n",
        "  for word in words:\n",
        "    word_embedding = model.encode(word, convert_to_tensor=True)\n",
        "  return word_embedding"
      ],
      "metadata": {
        "id": "AQuqeOXD18c4"
      },
      "id": "AQuqeOXD18c4",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consultar_modelo(key, entrada_prompt, esquema_extracao):\n",
        "    prompt = f\"\"\"\n",
        "      Você é um organizador de informações.\n",
        "      Dado o contexto: {key}\n",
        "      Com o esquema de extração: {esquema_extracao[key]}\n",
        "      E a informação extraída: {entrada_prompt}\n",
        "\n",
        "      Identifique se existe uma resposta válida nessa informação.\n",
        "      - Se existir, retorne apenas no formato: resposta\n",
        "      - Se não existir, retorne exatamente: null\n",
        "      \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-5-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Você é direto, objetivo e não inventa informações.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "8rhm3xccPvi2"
      },
      "id": "8rhm3xccPvi2",
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_with_cache(texts, model, cache):\n",
        "    texts_to_encode = [text for text in texts if text not in cache]\n",
        "    if texts_to_encode:\n",
        "        new_embeddings = model.encode(texts_to_encode, convert_to_tensor=True)\n",
        "        for text, emb in zip(texts_to_encode, new_embeddings):\n",
        "            cache[text] = emb\n",
        "    embeddings_list = [cache[text] for text in texts]\n",
        "    if embeddings_list:\n",
        "        return torch.stack(embeddings_list)\n",
        "    else:\n",
        "        return torch.tensor([])"
      ],
      "metadata": {
        "id": "0NWjXlx8DjU_"
      },
      "id": "0NWjXlx8DjU_",
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_cosine_sim(query_embedding, candidate_embeddings, candidates, n_neighbours, mapped_text_pos):\n",
        "  cosine_scores = util.cos_sim(query_embedding, candidate_embeddings)\n",
        "  ranked_answers = sorted(cosine_scores[0].tolist(), reverse=True)\n",
        "\n",
        "  ranked_results = sorted(\n",
        "      zip(candidates, cosine_scores[0].tolist()),\n",
        "      key=lambda x: x[1],\n",
        "      reverse=True\n",
        "  )\n",
        "  top_5_results = []\n",
        "  for candidate, score in ranked_results[:n_neighbours]:\n",
        "    top_5_results.append(candidate)\n",
        "\n",
        "  #Respostas mais próximas do alvo\n",
        "  best_results = find_closest_positions(top_5_results, mapped_text_pos, n_neighbours)\n",
        "  return best_results"
      ],
      "metadata": {
        "id": "poGqEJGTaA5Y"
      },
      "id": "poGqEJGTaA5Y",
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adicionar(saida_problema, label, pergunta, resposta):\n",
        "    saida_problema.setdefault(label, {})[pergunta] = resposta"
      ],
      "metadata": {
        "id": "AwQ-MrNxXuKz"
      },
      "id": "AwQ-MrNxXuKz",
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "id": "CStQTlDrLHd_",
      "metadata": {
        "id": "CStQTlDrLHd_"
      },
      "outputs": [],
      "source": [
        "def adicionar(saida_problema, label, arquivo_id, pergunta, resposta):\n",
        "    doc_dict = saida_problema.setdefault(label, {})\n",
        "    perguntas_dict = doc_dict.setdefault(arquivo_id, {})\n",
        "    perguntas_dict[pergunta] = resposta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def processar(input_path: str, output_path: str, k_neighbors: int):\n",
        "\n",
        "    data_json = {}\n",
        "    for file in glob.glob(os.path.join(Path(input_path), \"*.json\")):\n",
        "      with open(file, encoding='utf-8') as f:\n",
        "          dados = json.load(f)\n",
        "          data_json[Path(file).stem] = dados\n",
        "    labels_mapped_words = {}\n",
        "    embedding_cache = {}\n",
        "    saida_problema = {}\n",
        "\n",
        "    for file in data_json.keys():\n",
        "        archives = data_json[file]\n",
        "\n",
        "        candidate_embeddings = torch.tensor([])\n",
        "\n",
        "        for archive in tqdm.tqdm(archives, desc=f\"Processando {file}\"):\n",
        "\n",
        "            file_path = Path(archive[\"pdf_path\"])\n",
        "\n",
        "            textos_pdf = extrair_texto_arquivo(file_path)\n",
        "            textos_pdf = limpa_strings_simples(textos_pdf)\n",
        "\n",
        "            new_texts = map_labels_words(labels_mapped_words, archive[\"label\"], textos_pdf)\n",
        "\n",
        "            textos_pos = get_words_pos(file_path)\n",
        "\n",
        "            if len(new_texts) > 0:\n",
        "                new_embeddings = model.encode(new_texts, convert_to_tensor=True)\n",
        "\n",
        "                if candidate_embeddings.numel() == 0:\n",
        "                    candidate_embeddings = new_embeddings\n",
        "                else:\n",
        "                    candidate_embeddings = torch.cat((candidate_embeddings, new_embeddings), dim=0)\n",
        "\n",
        "            esquema_extracao = archive[\"extraction_schema\"]\n",
        "\n",
        "            for key in tqdm.tqdm(esquema_extracao.keys(), desc=f\"Processando Perguntas\"):\n",
        "                found = False\n",
        "\n",
        "                tensor_key = calc_embeddings([key])\n",
        "                best_ranked = calc_cosine_sim(tensor_key, candidate_embeddings, new_texts, k_neighbors, textos_pos)\n",
        "\n",
        "                strings_usadas = set()\n",
        "\n",
        "                for result in best_ranked.keys():\n",
        "                  entrada_prompt = best_ranked[result]\n",
        "\n",
        "                  entrada_filtrada = [item for item in entrada_prompt if item[0] not in strings_usadas]\n",
        "\n",
        "                  if not entrada_filtrada:\n",
        "                    continue\n",
        "\n",
        "                  strings_usadas.update(item[0] for item in entrada_filtrada)\n",
        "\n",
        "\n",
        "                  saida = consultar_modelo(key, entrada_filtrada, esquema_extracao)\n",
        "\n",
        "                  if saida != \"null\":\n",
        "                      adicionar(saida_problema, archive[\"label\"], archive[\"pdf_path\"], key, saida)\n",
        "                      found = True\n",
        "                      break\n",
        "\n",
        "                if not found:\n",
        "                    adicionar(saida_problema, archive[\"label\"], archive[\"pdf_path\"], key, None)\n",
        "\n",
        "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(saida_problema, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "u6VI8DFgeLef"
      },
      "id": "u6VI8DFgeLef",
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Processador de documentos JSON com GPT.\")\n",
        "    parser.add_argument(\"--input\", required=True, help=\"Caminho dos JSONs de entrada.\")\n",
        "    parser.add_argument(\"--output\", required=True, help=\"Arquivo JSON para salvar os resultados.\")\n",
        "    parser.add_argument(\"--neighbors\", type=int, default=5, help=\"Número de vizinhos para cosine similarity.\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    processar(args.input, args.output, args.neighbors)"
      ],
      "metadata": {
        "id": "a9R3aSKUUvkJ"
      },
      "id": "a9R3aSKUUvkJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processar(\"./jsons_path/\", \"saida.json\", 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oXdk4l4pnYJ",
        "outputId": "3c898e91-865a-418b-c13e-3bdc8bcceb92"
      },
      "id": "9oXdk4l4pnYJ",
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando dataset:   0%|          | 0/6 [00:00<?, ?it/s]\n",
            "Processando Perguntas:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Processando Perguntas:  12%|█▎        | 1/8 [00:14<01:40, 14.42s/it]\u001b[A\n",
            "Processando Perguntas:  25%|██▌       | 2/8 [00:19<00:52,  8.68s/it]\u001b[A\n",
            "Processando Perguntas:  38%|███▊      | 3/8 [00:24<00:35,  7.18s/it]\u001b[A\n",
            "Processando Perguntas:  50%|█████     | 4/8 [00:36<00:36,  9.00s/it]\u001b[A\n",
            "Processando Perguntas:  62%|██████▎   | 5/8 [00:49<00:31, 10.46s/it]\u001b[A\n",
            "Processando Perguntas:  75%|███████▌  | 6/8 [01:03<00:23, 11.66s/it]\u001b[A\n",
            "Processando Perguntas:  88%|████████▊ | 7/8 [01:22<00:14, 14.08s/it]\u001b[A\n",
            "Processando Perguntas: 100%|██████████| 8/8 [01:36<00:00, 12.06s/it]\n",
            "Processando dataset:  17%|█▋        | 1/6 [01:37<08:09, 97.98s/it]\n",
            "Processando Perguntas:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "Processando Perguntas:  14%|█▍        | 1/7 [00:06<00:36,  6.17s/it]\u001b[A\n",
            "Processando Perguntas:  29%|██▊       | 2/7 [00:11<00:29,  5.90s/it]\u001b[A\n",
            "Processando Perguntas:  43%|████▎     | 3/7 [00:17<00:22,  5.70s/it]\u001b[A\n",
            "Processando Perguntas:  57%|█████▋    | 4/7 [00:24<00:19,  6.39s/it]\u001b[A\n",
            "Processando Perguntas:  71%|███████▏  | 5/7 [00:28<00:10,  5.28s/it]\u001b[A\n",
            "Processando Perguntas:  86%|████████▌ | 6/7 [00:32<00:04,  4.90s/it]\u001b[A\n",
            "Processando Perguntas: 100%|██████████| 7/7 [00:38<00:00,  5.44s/it]\n",
            "Processando dataset:  33%|███▎      | 2/6 [02:16<04:11, 62.93s/it]\n",
            "Processando Perguntas:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "Processando Perguntas:  14%|█▍        | 1/7 [00:17<01:42, 17.15s/it]\u001b[A\n",
            "Processando Perguntas:  29%|██▊       | 2/7 [00:22<00:51, 10.39s/it]\u001b[A\n",
            "Processando Perguntas:  43%|████▎     | 3/7 [00:29<00:35,  8.85s/it]\u001b[A\n",
            "Processando Perguntas:  57%|█████▋    | 4/7 [00:48<00:38, 12.72s/it]\u001b[A\n",
            "Processando Perguntas:  71%|███████▏  | 5/7 [00:58<00:23, 11.57s/it]\u001b[A\n",
            "Processando Perguntas:  86%|████████▌ | 6/7 [01:09<00:11, 11.51s/it]\u001b[A\n",
            "Processando Perguntas: 100%|██████████| 7/7 [01:14<00:00, 10.61s/it]\n",
            "Processando dataset:  50%|█████     | 3/6 [03:30<03:24, 68.27s/it]\n",
            "Processando Perguntas:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\n",
            "Processando Perguntas:  14%|█▍        | 1/7 [00:05<00:32,  5.43s/it]\u001b[A\n",
            "Processando Perguntas:  29%|██▊       | 2/7 [00:14<00:37,  7.43s/it]\u001b[A\n",
            "Processando Perguntas:  43%|████▎     | 3/7 [00:35<00:54, 13.53s/it]\u001b[A\n",
            "Processando Perguntas:  57%|█████▋    | 4/7 [00:59<00:53, 17.73s/it]\u001b[A\n",
            "Processando Perguntas:  71%|███████▏  | 5/7 [01:11<00:31, 15.80s/it]\u001b[A\n",
            "Processando Perguntas:  86%|████████▌ | 6/7 [01:20<00:13, 13.30s/it]\u001b[A\n",
            "Processando Perguntas: 100%|██████████| 7/7 [01:30<00:00, 12.97s/it]\n",
            "Processando dataset:  67%|██████▋   | 4/6 [05:03<02:35, 77.72s/it]\n",
            "Processando Perguntas:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            "Processando Perguntas:  20%|██        | 1/5 [00:11<00:44, 11.05s/it]\u001b[A\n",
            "Processando Perguntas:  40%|████      | 2/5 [00:26<00:41, 13.70s/it]\u001b[A\n",
            "Processando Perguntas:  60%|██████    | 3/5 [00:34<00:22, 11.23s/it]\u001b[A\n",
            "Processando Perguntas:  80%|████████  | 4/5 [00:53<00:14, 14.25s/it]\u001b[A\n",
            "Processando Perguntas: 100%|██████████| 5/5 [01:09<00:00, 13.87s/it]\n",
            "Processando dataset:  83%|████████▎ | 5/6 [06:14<01:15, 75.28s/it]\n",
            "Processando Perguntas:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Processando Perguntas:  33%|███▎      | 1/3 [00:14<00:29, 14.92s/it]\u001b[A\n",
            "Processando Perguntas:  67%|██████▋   | 2/3 [00:34<00:17, 17.86s/it]\u001b[A\n",
            "Processando Perguntas: 100%|██████████| 3/3 [00:47<00:00, 15.69s/it]\n",
            "Processando dataset: 100%|██████████| 6/6 [07:02<00:00, 70.38s/it]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "enterai_env",
      "language": "python",
      "name": "enterai_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}